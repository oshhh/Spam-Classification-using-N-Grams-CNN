{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,) (1393,)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 162)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 162)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 162)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 162, 100)     758100      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 162, 100)     758100      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 162, 100)     758100      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 155, 32)      25632       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 157, 32)      19232       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 159, 32)      12832       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 155, 32)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 157, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 159, 32)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 77, 32)       0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 78, 32)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 79, 32)       0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2464)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2496)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2528)         0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 7488)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           74890       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,406,897\n",
      "Trainable params: 2,406,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "4179/4179 [==============================] - 11s 3ms/step - loss: 0.1884 - acc: 0.9414\n",
      "Epoch 2/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 0.0993 - acc: 0.9909\n",
      "Epoch 3/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 0.0768 - acc: 0.9964\n",
      "Epoch 4/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 0.0103 - acc: 0.9974\n",
      "Epoch 5/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 6/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 2.8776e-04 - acc: 1.0000\n",
      "Epoch 7/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 1.0665e-04 - acc: 1.0000\n",
      "Epoch 8/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 3.2453e-04 - acc: 0.9998\n",
      "Epoch 9/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 3.8405e-05 - acc: 1.0000\n",
      "Epoch 10/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 1.6236e-05 - acc: 1.0000\n",
      "Epoch 11/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 4.4351e-05 - acc: 1.0000\n",
      "Epoch 12/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 2.1630e-05 - acc: 1.0000\n",
      "Epoch 13/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 2.4385e-04 - acc: 1.0000\n",
      "Epoch 14/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 3.0230e-06 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 3.4272e-06 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 2.0552e-06 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 1.4401e-06 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 1.1119e-06 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 3.2849e-06 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "4179/4179 [==============================] - 10s 2ms/step - loss: 6.1059e-05 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from numpy import array\n",
    "\n",
    "def get_data(train_file, test_file = None):\n",
    "    if test_file == None:\n",
    "        frame = pd.read_csv(train_file)\n",
    "        data = frame.values\n",
    "        np.random.shuffle(data)\n",
    "        return data\n",
    "    else:\n",
    "        train_frame = pd.read_csv(train_file)\n",
    "        test_frame = pd.read_csv(test_file)\n",
    "\n",
    "        train_data = train_frame.values\n",
    "        test_data = test_frame.values\n",
    "        np.random.shuffle(train_data)\n",
    "        np.random.shuffle(test_data)\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "def get_training_testing_sets(train_file, test_file = None):\n",
    "    if test_file == None:\n",
    "        data = get_data(train_file)\n",
    "        train_data, test_data = train_test_split(data)\n",
    "    else:\n",
    "\n",
    "        train_data, test_data = get_data(train_file, test_file)\n",
    "\n",
    "    X_train = train_data[:, 1]\n",
    "    Y_train = train_data[:, 0]\n",
    "    X_test = test_data[:, 1]\n",
    "    Y_test = test_data[:, 0]\n",
    "\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "def get_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def max_length(lines):\n",
    "    return max([len(sentence.split()) for sentence in lines])\n",
    "\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "def define_model(length, vocab_size, channels, kernel_size):\n",
    "    inputs = {}\n",
    "    embedding = {}\n",
    "    conv = {}\n",
    "    drop = {}\n",
    "    pool = {}\n",
    "    flat = {}\n",
    "    for channel in range(1, channels + 1):\n",
    "        inputs[channel] = Input(shape = (length,))\n",
    "        embedding[channel] = Embedding(vocab_size, 100)(inputs[channel])\n",
    "        conv[channel] = Conv1D(filters = 32, kernel_size = kernel_size[channel], activation = 'relu')(embedding[channel])\n",
    "        drop[channel] = Dropout(0.5)(conv[channel])\n",
    "        pool[channel] = MaxPooling1D(pool_size = 2)(drop[channel])\n",
    "        flat[channel] = Flatten()(pool[channel])\n",
    "    merged = concatenate(list(flat.values()))\n",
    "    dense = Dense(10, activation = 'relu')(merged)\n",
    "    outputs = Dense(1, activation = 'sigmoid')(dense)\n",
    "    \n",
    "    model = Model(list(inputs.values()), outputs=outputs)\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes = True, to_file = 'multichannel.png')\n",
    "    return model\n",
    "\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = get_training_testing_sets('../input/SPAM text message 20170820 - Data.csv')\n",
    "for i in range(Y_train.shape[0]):\n",
    "    Y_train[i] = (Y_train[i] == 'spam')\n",
    "\n",
    "for i in range(Y_test.shape[0]):\n",
    "    Y_test[i] = (Y_test[i] == 'spam')\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer(X_train)\n",
    "length = max_length(X_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X_train = encode_text(tokenizer, X_train, length)\n",
    "model = define_model(length, vocab_size, 3, {1 : 8, 2 : 6, 3 : 4})\n",
    "model.fit([X_train, X_train, X_train], array(Y_train), epochs = 20, batch_size = 16)\n",
    "\n",
    "tokenizer = get_tokenizer(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X_test = encode_text(tokenizer, X_test, length)\n",
    "loss, acc = model.evaluate([X_test,X_test,X_test],array(Y_test), verbose=0)\n",
    "\n",
    "model.save('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8628858579035209\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
